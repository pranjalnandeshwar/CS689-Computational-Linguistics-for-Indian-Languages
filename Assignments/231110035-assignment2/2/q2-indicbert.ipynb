{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7810492,"sourceType":"datasetVersion","datasetId":4574610}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Marathi Dataset\n\nThe dataset for marathi language is directly fetched from the website.","metadata":{}},{"cell_type":"code","source":"!pip install datasets\nfrom datasets import ClassLabel, Sequence, Dataset, Value, Features, load_dataset, load_metric\nraw_datasets = load_dataset('ai4bharat/naamapadam','mr')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:24:39.963995Z","iopub.execute_input":"2024-03-13T12:24:39.964401Z","iopub.status.idle":"2024-03-13T12:24:54.755514Z","shell.execute_reply.started":"2024-03-13T12:24:39.964357Z","shell.execute_reply":"2024-03-13T12:24:54.754636Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64628ea5d0e045a4b22b028b125256f8"}},"metadata":{}}]},{"cell_type":"code","source":"features = raw_datasets[\"train\"].features\nlabel_list = features['ner_tags'].feature.names\nlabel_to_id = {label_list[i]: features['ner_tags'].feature.str2int( label_list[i] ) for i in range(len(label_list))}\nprint(label_to_id)\nnum_labels = len(label_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:24:54.757818Z","iopub.execute_input":"2024-03-13T12:24:54.758628Z","iopub.status.idle":"2024-03-13T12:24:54.765621Z","shell.execute_reply.started":"2024-03-13T12:24:54.758593Z","shell.execute_reply":"2024-03-13T12:24:54.764718Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# FineTuning IndicBERT for NER classification\n\nFirstly, we need to tokenize the training and validation dataset and then map the words to their NER labels.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\nimport numpy as np\n\nconfig = AutoConfig.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels, finetuning_task='ner')\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\nmodel = AutoModelForTokenClassification.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels )","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:24:54.766927Z","iopub.execute_input":"2024-03-13T12:24:54.767215Z","iopub.status.idle":"2024-03-13T12:25:04.255170Z","shell.execute_reply.started":"2024-03-13T12:24:54.767192Z","shell.execute_reply":"2024-03-13T12:25:04.254041Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-03-13 12:24:59.136213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-13 12:24:59.136272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-13 12:24:59.137883: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model=model.to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:04.256538Z","iopub.execute_input":"2024-03-13T12:25:04.257115Z","iopub.status.idle":"2024-03-13T12:25:04.407734Z","shell.execute_reply.started":"2024-03-13T12:25:04.257086Z","shell.execute_reply":"2024-03-13T12:25:04.406824Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Tokenize all texts and align the labels with them.\npadding = \"max_length\"\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples['tokens'],\n        padding=padding,\n        truncation=True,\n        max_length=512,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    labels = []\n    for i, label in enumerate(examples['ner_tags']):      \n        word_ids = tokenized_inputs.word_ids(batch_index=i) \n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:      \n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:04.410176Z","iopub.execute_input":"2024-03-13T12:25:04.410514Z","iopub.status.idle":"2024-03-13T12:25:04.418688Z","shell.execute_reply.started":"2024-03-13T12:25:04.410457Z","shell.execute_reply":"2024-03-13T12:25:04.417605Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing train_dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = raw_datasets[\"train\"].select(range(20000))   #sampling\n\ntrain_dataset = train_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on train dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:04.420117Z","iopub.execute_input":"2024-03-13T12:25:04.420777Z","iopub.status.idle":"2024-03-13T12:25:10.803206Z","shell.execute_reply.started":"2024-03-13T12:25:04.420740Z","shell.execute_reply":"2024-03-13T12:25:10.802065Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #0:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c44fcc1719437b9f7d8df47682129d"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #1:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ea1f3cb56c049cb98a1a3fc5780c78e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #2:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc633528adb24225834d1919b5ea9f92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #3:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40129820ea84a8aaee7ba448cbb3768"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing validation_dataset","metadata":{}},{"cell_type":"code","source":"eval_dataset = raw_datasets[\"validation\"]        #no-sampling\neval_dataset = eval_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on Validation dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:10.804806Z","iopub.execute_input":"2024-03-13T12:25:10.805115Z","iopub.status.idle":"2024-03-13T12:25:11.869989Z","shell.execute_reply.started":"2024-03-13T12:25:10.805086Z","shell.execute_reply":"2024-03-13T12:25:11.868824Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65acf742e0b2414f9c0b21916f11a31e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4221e9f42c74a6886214ca8215d0cac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16951c9b073d49bfbe640a970d3ef7f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f6ca0545f6743c0b846250f08222962"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:11.871685Z","iopub.execute_input":"2024-03-13T12:25:11.872030Z","iopub.status.idle":"2024-03-13T12:25:11.876852Z","shell.execute_reply.started":"2024-03-13T12:25:11.871998Z","shell.execute_reply":"2024-03-13T12:25:11.875736Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Designing a compute metrics\n\nWe require compute metrics to evaluate the model. I have retrieved precision, recall, f1-score and accuracy here. Using these values, I will calculate the macro-f1 score.","metadata":{}},{"cell_type":"code","source":"!pip install seqeval\n\ndef all_classes(true_tags, pred_tags):\n    classes = set(tag for row in true_tags + pred_tags for tag in row)\n\n    results = []\n    true_positives, false_positives, false_negatives = 0, 0, 0\n    for class_label in sorted(classes):\n        true_indices = [(i, j) for i, row in enumerate(true_tags) for j, tag in enumerate(row) if tag == class_label]\n        pred_indices = [(i, j) for i, row in enumerate(pred_tags) for j, tag in enumerate(row) if tag == class_label]\n        \n        true_positive = len(set(true_indices) & set(pred_indices))\n        false_positive = len(set(pred_indices) - set(true_indices))\n        false_negative = len(set(true_indices) - set(pred_indices))\n\n        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n        results.append((class_label, precision, recall, f1_score))\n\n        true_positives += true_positive\n        false_positives += false_positive\n        false_negatives += false_negative\n\n    macro_f1_score = 2 * (true_positives) / (2 * true_positives + false_positives + false_negatives) if (2 * true_positives + false_positives + false_negatives) > 0 else 0\n    results.append(('macro f1:', 0, 0, macro_f1_score))\n\n    return results\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:11.878254Z","iopub.execute_input":"2024-03-13T12:25:11.878638Z","iopub.status.idle":"2024-03-13T12:25:24.733051Z","shell.execute_reply.started":"2024-03-13T12:25:11.878605Z","shell.execute_reply":"2024-03-13T12:25:24.731928Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Metrics function\nmetric = load_metric(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    r1 = all_classes(true_labels, true_predictions)\n    print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n    for class_label, precision, recall, f1_score in r1:\n        print(f\"{class_label}\\t{precision:.2f}\\t\\t{recall:.2f}\\t{f1_score:.2f}\")\n        \n    results = metric.compute(predictions=true_predictions, references=true_labels)\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:24.734599Z","iopub.execute_input":"2024-03-13T12:25:24.734913Z","iopub.status.idle":"2024-03-13T12:25:24.934489Z","shell.execute_reply.started":"2024-03-13T12:25:24.734884Z","shell.execute_reply":"2024-03-13T12:25:24.933633Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:24.935647Z","iopub.execute_input":"2024-03-13T12:25:24.935909Z","iopub.status.idle":"2024-03-13T12:25:38.497615Z","shell.execute_reply.started":"2024-03-13T12:25:24.935886Z","shell.execute_reply":"2024-03-13T12:25:38.496283Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nCollecting accelerate\n  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nUsing cached accelerate-0.28.0-py3-none-any.whl (290 kB)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.27.2\n    Uninstalling accelerate-0.27.2:\n      Successfully uninstalled accelerate-0.27.2\nSuccessfully installed accelerate-0.28.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Finetuning the hyper-parameters\n\nFor fine-tuning a pre-trained model, we need to tune the hyper-parameters which are mentioned in the TrainingArguments. If a particular hyper parameter is not mentioned then it will take the default value.","metadata":{}},{"cell_type":"code","source":"args=TrainingArguments(\n    output_dir='Trained Models/IndicBERT/Fine_tuned_IndicBERT',\n    num_train_epochs=3,\n    logging_steps=500, #500\n    save_steps=1000, #1000\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:38.499165Z","iopub.execute_input":"2024-03-13T12:25:38.499501Z","iopub.status.idle":"2024-03-13T12:25:38.513538Z","shell.execute_reply.started":"2024-03-13T12:25:38.499453Z","shell.execute_reply":"2024-03-13T12:25:38.512524Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training the model\n\nAs training was taking more time and due to limited resources, I sampled off the training dataset. Also, I discussed this issue with my colleagues. They were getting different EFT (Expected Finish Time).","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,    #.select(range(20000)), #sampling\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    args=args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:38.514782Z","iopub.execute_input":"2024-03-13T12:25:38.515060Z","iopub.status.idle":"2024-03-13T12:25:39.157207Z","shell.execute_reply.started":"2024-03-13T12:25:38.515036Z","shell.execute_reply":"2024-03-13T12:25:39.156370Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#GPU 100,000 -> 12h , 10k -> 3h, entire dataset -> 61h, 25% -> 15h\n#GPU Kaggle p100: 100,000 -> 7h,\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:25:39.161029Z","iopub.execute_input":"2024-03-13T12:25:39.161304Z","iopub.status.idle":"2024-03-13T13:17:40.422970Z","shell.execute_reply.started":"2024-03-13T12:25:39.161281Z","shell.execute_reply":"2024-03-13T13:17:40.421389Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240313_122547-p28m7w61</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/self-team-it/huggingface/runs/p28m7w61' target=\"_blank\">treasured-fire-15</a></strong> to <a href='https://wandb.ai/self-team-it/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/self-team-it/huggingface' target=\"_blank\">https://wandb.ai/self-team-it/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/self-team-it/huggingface/runs/p28m7w61' target=\"_blank\">https://wandb.ai/self-team-it/huggingface/runs/p28m7w61</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3750/3750 51:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.665600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.458200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.405500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.360800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.357200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.294100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.280900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3750, training_loss=0.39502447509765626, metrics={'train_runtime': 3120.8936, 'train_samples_per_second': 19.225, 'train_steps_per_second': 1.202, 'total_flos': 1325736898560000.0, 'train_loss': 0.39502447509765626, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(train_dataset)\n#takes time","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:17:40.424559Z","iopub.execute_input":"2024-03-13T13:17:40.424982Z","iopub.status.idle":"2024-03-13T13:24:40.274566Z","shell.execute_reply.started":"2024-03-13T13:17:40.424943Z","shell.execute_reply":"2024-03-13T13:24:40.273436Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.81\t\t0.86\t0.84\nB-ORG\t0.82\t\t0.73\t0.77\nB-PER\t0.87\t\t0.85\t0.86\nI-LOC\t0.73\t\t0.52\t0.61\nI-ORG\t0.80\t\t0.65\t0.71\nI-PER\t0.89\t\t0.87\t0.88\nO\t0.95\t\t0.97\t0.96\nmacro f1:\t0.00\t\t0.00\t0.92\n","output_type":"stream"},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.7891179918626302, 'recall': 0.8429776419020958, 'f1': 0.8151591245035706, 'number': 27149}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7646996391424326, 'recall': 0.696606400464082, 'f1': 0.7290665317480394, 'number': 20686}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8435242681535927, 'recall': 0.8291809405169729, 'f1': 0.8362911078305117, 'number': 32110}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.25029969215393066,\n 'eval_LOC': {'precision': 0.7891179918626302,\n  'recall': 0.8429776419020958,\n  'f1': 0.8151591245035706,\n  'number': 27149},\n 'eval_ORG': {'precision': 0.7646996391424326,\n  'recall': 0.696606400464082,\n  'f1': 0.7290665317480394,\n  'number': 20686},\n 'eval_PER': {'precision': 0.8435242681535927,\n  'recall': 0.8291809405169729,\n  'f1': 0.8362911078305117,\n  'number': 32110},\n 'eval_overall_precision': 0.8049489988666415,\n 'eval_overall_recall': 0.7995621990118206,\n 'eval_overall_f1': 0.8022465564306109,\n 'eval_overall_accuracy': 0.9247723389285355,\n 'eval_runtime': 419.7984,\n 'eval_samples_per_second': 47.642,\n 'eval_steps_per_second': 2.978,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"code","source":"#GPU\ntrainer.evaluate() #evaluation values\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:24:40.276024Z","iopub.execute_input":"2024-03-13T13:24:40.276336Z","iopub.status.idle":"2024-03-13T13:25:28.572498Z","shell.execute_reply.started":"2024-03-13T13:24:40.276307Z","shell.execute_reply":"2024-03-13T13:25:28.571236Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.75\t\t0.78\t0.76\nB-ORG\t0.72\t\t0.68\t0.70\nB-PER\t0.81\t\t0.79\t0.80\nI-LOC\t0.59\t\t0.46\t0.52\nI-ORG\t0.63\t\t0.52\t0.57\nI-PER\t0.84\t\t0.83\t0.83\nO\t0.93\t\t0.95\t0.94\nmacro f1:\t0.00\t\t0.00\t0.89\n","output_type":"stream"},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.7256116444719728, 'recall': 0.7533762057877813, 'f1': 0.7392333175579744, 'number': 3110}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6663996796155387, 'recall': 0.6457120682964688, 'f1': 0.6558927867560111, 'number': 2577}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7661768497035318, 'recall': 0.7533586818757921, 'f1': 0.7597137014314929, 'number': 3945}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.36112767457962036,\n 'eval_LOC': {'precision': 0.7256116444719728,\n  'recall': 0.7533762057877813,\n  'f1': 0.7392333175579744,\n  'number': 3110},\n 'eval_ORG': {'precision': 0.6663996796155387,\n  'recall': 0.6457120682964688,\n  'f1': 0.6558927867560111,\n  'number': 2577},\n 'eval_PER': {'precision': 0.7661768497035318,\n  'recall': 0.7533586818757921,\n  'f1': 0.7597137014314929,\n  'number': 3945},\n 'eval_overall_precision': 0.72660072878709,\n 'eval_overall_recall': 0.7245639534883721,\n 'eval_overall_f1': 0.7255809117845817,\n 'eval_overall_accuracy': 0.8897393498235625,\n 'eval_runtime': 48.27,\n 'eval_samples_per_second': 47.649,\n 'eval_steps_per_second': 2.983,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"Fine_tuned_IndicBERT\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:25:28.573872Z","iopub.execute_input":"2024-03-13T13:25:28.574203Z","iopub.status.idle":"2024-03-13T13:25:29.106172Z","shell.execute_reply.started":"2024-03-13T13:25:28.574175Z","shell.execute_reply":"2024-03-13T13:25:29.104870Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Test Phase","metadata":{}},{"cell_type":"code","source":"#TEST\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/Fine_tuned_IndicBERT\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/Fine_tuned_IndicBERT\")\n\ntest_dataset = raw_datasets[\"test\"]\ntest_dataset = test_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on test dataset\",\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:25:29.108070Z","iopub.execute_input":"2024-03-13T13:25:29.109308Z","iopub.status.idle":"2024-03-13T13:25:30.874006Z","shell.execute_reply.started":"2024-03-13T13:25:29.109259Z","shell.execute_reply":"2024-03-13T13:25:30.871950Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/Fine_tuned_IndicBERT and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed39e4a655fd420989a3fce54e269e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24cbf6f99403462494a8cb6ae0c40f37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e75ea8d283914942bb01151366e4637e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"259f93452b6645ad94e3a906004eecd1"}},"metadata":{}}]},{"cell_type":"code","source":"x = trainer.predict(test_dataset)\nx.metrics","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:25:30.875593Z","iopub.execute_input":"2024-03-13T13:25:30.875929Z","iopub.status.idle":"2024-03-13T13:25:53.500139Z","shell.execute_reply.started":"2024-03-13T13:25:30.875897Z","shell.execute_reply":"2024-03-13T13:25:53.498934Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.85\t\t0.74\t0.79\nB-ORG\t0.78\t\t0.68\t0.73\nB-PER\t0.86\t\t0.80\t0.83\nI-LOC\t0.62\t\t0.50\t0.55\nI-ORG\t0.62\t\t0.52\t0.57\nI-PER\t0.89\t\t0.86\t0.87\nO\t0.93\t\t0.97\t0.95\nmacro f1:\t0.00\t\t0.00\t0.90\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.306081622838974,\n 'test_LOC': {'precision': 0.8152173913043478,\n  'recall': 0.7239382239382239,\n  'f1': 0.7668711656441718,\n  'number': 1554},\n 'test_ORG': {'precision': 0.7149576669802445,\n  'recall': 0.63919259882254,\n  'f1': 0.674955595026643,\n  'number': 1189},\n 'test_PER': {'precision': 0.8278628738147338,\n  'recall': 0.7731607629427792,\n  'f1': 0.7995773159563226,\n  'number': 1468},\n 'test_overall_precision': 0.7918196119559517,\n 'test_overall_recall': 0.7171693184516742,\n 'test_overall_f1': 0.7526479750778815,\n 'test_overall_accuracy': 0.9039969802457745,\n 'test_runtime': 22.6054,\n 'test_samples_per_second': 47.776,\n 'test_steps_per_second': 3.008}"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:26:29.180020Z","iopub.execute_input":"2024-03-13T13:26:29.180942Z","iopub.status.idle":"2024-03-13T13:27:28.924756Z","shell.execute_reply.started":"2024-03-13T13:26:29.180902Z","shell.execute_reply":"2024-03-13T13:27:28.923358Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"updating: kaggle/working/ (stored 0%)\nupdating: kaggle/working/.virtual_documents/ (stored 0%)\nupdating: kaggle/working/wandb/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/output.log (deflated 87%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/config.yaml (deflated 77%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/wandb-metadata.json (deflated 65%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/wandb-summary.json (deflated 53%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/conda-environment.yaml (deflated 67%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/files/requirements.txt (deflated 58%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/logs/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/logs/debug-internal.log (deflated 94%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/logs/debug.log (deflated 75%)\nupdating: kaggle/working/wandb/run-20240312_153406-7cm37hdf/run-7cm37hdf.wandb (deflated 82%)\nupdating: kaggle/working/wandb/debug-internal.log (deflated 94%)\nupdating: kaggle/working/wandb/debug.log (deflated 71%)\nupdating: kaggle/working/wandb/latest-run/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/tmp/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/tmp/code/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/files/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/files/output.log (deflated 69%)\nupdating: kaggle/working/wandb/latest-run/files/config.yaml (deflated 78%)\nupdating: kaggle/working/wandb/latest-run/files/wandb-metadata.json (deflated 65%)\nupdating: kaggle/working/wandb/latest-run/files/wandb-summary.json (deflated 54%)\nupdating: kaggle/working/wandb/latest-run/files/conda-environment.yaml (deflated 67%)\nupdating: kaggle/working/wandb/latest-run/files/requirements.txt (deflated 58%)\nupdating: kaggle/working/wandb/latest-run/logs/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/logs/debug-internal.log (deflated 94%)\nupdating: kaggle/working/wandb/latest-run/logs/debug.log (deflated 71%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/ (stored 0%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/tokenizer_config.json (deflated 74%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/model.safetensors (deflated 7%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/spiece.model (deflated 60%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/config.json (deflated 56%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/special_tokens_map.json (deflated 49%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/training_args.bin (deflated 51%)\nupdating: kaggle/working/Fine_tuned_IndicBERT/tokenizer.json (deflated 77%)\nupdating: kaggle/working/Trained Models/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/trainer_state.json (deflated 67%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/tokenizer_config.json (deflated 74%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/model.safetensors (deflated 7%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/spiece.model (deflated 60%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/scheduler.pt (deflated 55%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/optimizer.pt (deflated 79%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/config.json (deflated 56%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/special_tokens_map.json (deflated 49%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/training_args.bin (deflated 51%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/tokenizer.json (deflated 77%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-3000/rng_state.pth (deflated 25%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/trainer_state.json (deflated 55%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/tokenizer_config.json (deflated 74%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/model.safetensors (deflated 7%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/spiece.model (deflated 60%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/scheduler.pt (deflated 56%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/optimizer.pt (deflated 79%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/config.json (deflated 56%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/special_tokens_map.json (deflated 49%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/training_args.bin (deflated 51%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/tokenizer.json (deflated 77%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-1000/rng_state.pth (deflated 25%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/trainer_state.json (deflated 62%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/tokenizer_config.json (deflated 74%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/model.safetensors (deflated 7%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/spiece.model (deflated 60%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/scheduler.pt (deflated 55%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/optimizer.pt (deflated 79%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/config.json (deflated 56%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/special_tokens_map.json (deflated 49%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/training_args.bin (deflated 51%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/tokenizer.json (deflated 77%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/checkpoint-2000/rng_state.pth (deflated 25%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar12_15-33-14_d162eb94370d/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar12_15-33-14_d162eb94370d/events.out.tfevents.1710261166.d162eb94370d.34.1 (deflated 48%)\nupdating: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar12_15-33-14_d162eb94370d/events.out.tfevents.1710257636.d162eb94370d.34.0 (deflated 60%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/run-p28m7w61.wandb (deflated 82%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/requirements.txt (deflated 58%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/wandb-summary.json (deflated 54%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/conda-environment.yaml (deflated 67%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/output.log (deflated 84%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/config.yaml (deflated 78%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/files/wandb-metadata.json (deflated 65%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/logs/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/logs/debug-internal.log (deflated 94%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/logs/debug.log (deflated 71%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240313_122547-p28m7w61/tmp/code/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/run-p28m7w61.wandb (deflated 82%)\n  adding: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar13_12-25-38_c6028ab50de2/ (stored 0%)\n  adding: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar13_12-25-38_c6028ab50de2/events.out.tfevents.1710336280.c6028ab50de2.571.1 (deflated 49%)\n  adding: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar13_12-25-38_c6028ab50de2/events.out.tfevents.1710332739.c6028ab50de2.571.0 (deflated 60%)\n  adding: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar13_12-24-30_c6028ab50de2/ (stored 0%)\n  adding: kaggle/working/Trained Models/IndicBERT/Fine_tuned_IndicBERT/runs/Mar13_12-24-30_c6028ab50de2/events.out.tfevents.1710332672.c6028ab50de2.34.0 (deflated 61%)\n  adding: kaggle/working/state.db (deflated 14%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:27:28.927335Z","iopub.execute_input":"2024-03-13T13:27:28.927694Z","iopub.status.idle":"2024-03-13T13:27:30.022523Z","shell.execute_reply.started":"2024-03-13T13:27:28.927660Z","shell.execute_reply":"2024-03-13T13:27:30.021081Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":" Fine_tuned_IndicBERT  'Trained Models'   file.zip   state.db   wandb\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:27:34.175306Z","iopub.execute_input":"2024-03-13T13:27:34.176256Z","iopub.status.idle":"2024-03-13T13:27:34.185099Z","shell.execute_reply.started":"2024-03-13T13:27:34.176215Z","shell.execute_reply":"2024-03-13T13:27:34.183957Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Q4 Appended.\n\nThe part of Question 4 wherein we have to compute precision recall f1-score on the fine-tuned IndicBERT has been appended here.","metadata":{}},{"cell_type":"code","source":"Q1_data = {\n    'tokens':[[\"कार्यकर्त्यांमधील\", \"वाद\", \"बघून\", \"संतापलेल्या\", \"सुप्रिया\", \"सुळे\", \"यांनी\", \"कार्यक्रमामध्येच\", \"राडेबाज\", \"कार्यकर्त्यांना\",\"दम\", \"दिला\", \".\"],\n              [\"अर्थात\", \"यात\", \"काही\", \"चांगल्या\", \"आणि\", \"व्हिडिओला\", \"पाठिंबा\", \"देणाऱ्या\", \"कमेंटही\", \"होत्या\", \".\"],\n              [\"भारतीय\", \"वायुसेनेनं\", \"केलेल्या\", \"कारवाईचं\", \"संपुर्ण\", \"देशभरात\", \"कौतूक\", \"केलं\", \"जात\", \"आहे\", \".\"],\n              [\"24)\", \"बैठक\", \"बोलावली\", \"असल्याची\", \"माहिती\", \"आमदार\", \"अनिल\", \"बाबर\", \"यांनी\",\".\", \".\", \".\", \"ढवळीत\",\"पुरग्रस्तांच्या\",\"मदत\", \"वाटपाची\", \"चौकशी\", \"सुरु\"],\n              [\"यावेळी\", \"सुरेश\", \"प्रभुनी\", \"सुधीर\", \"सावंत\", \"व\", \"मधु\", \"दंडवते\", \"या\", \"दोघांचाही\", \"पराभव\", \"केला\", \".\", \"त्यावेळी\", \"निकालादिवशी\", \"सुरेश\", \"प्रभुंना\", \"विजयी\", \"घोषित\" \"केल्यानंतर\", \"त्यांनी\", \"लगेचच\", \"शेजारी\", \"उभे\", \"असलेल्या\", \"मधु\", \"दंडवतेंची\", \"भेट\", \"घेत\", \"वाकुन\", \"त्यांच्या\", \"चरणांना\", \"स्पर्श\", \"केला\", \".\", \"मतांच्या\", \"गोळाबेरजेत\", \"आपण\", \"विजयी\", \"झालेलो\", \"असलो\", \"तरी\", \"मधु\", \"दंडवतें\", \"या\", \"व्यक्तीचे\", \"कोकणप्रती\", \"असलेले\", \"योगदान\", \"प्रभुना\", \"चांगलेच\", \"ठाऊक\", \"होते\", \".\"],\n              [\"सव्वाशे\", \"वर्षे\", \"अखंड\", \"पाणीपुरवठा\", \":\", \"संस्थानकालीन\", \"योजना\", \":\", \"सुधारित\", \"45\", \"कोटींचा\", \"आराखडा\", \"लवकरच\", \"मंजूर\"],\n              [\"कार्यक्रम\", \"सर्वांना\", \"खुला\", \"असून\", \",\", \"रसिकांनी\", \"उपस्थित\", \"रहावे\", \"असे\", \"आवाहन\", \"लोकमान्य\", \"ग्रंथालयाने\", \"केले\", \"आहे\", \".\"],\n              [\"विमोचन\", \"आणि\", \"तो\", \"एक\", \"पादचारी\", \"दाबा\", \"आणि\", \"एक\", \"कार\", \"दाबा\", \"होते\", \"तेव्हा\", \",\", \"उदाहरणार्थ\", \",\", \"गुन्हा\", \"मागोवा\", \"लपविण्यासाठी\", \"मानले\", \"जाते\", \".\"],\n              [\"निसर्गराजा\", \"मित्र\", \"जीवांचे\", \"ही\", \"संस्था\", \"गेली\", \"दहा\", \"वर्षे\", \"पिंपरी\", \"-\", \"चिंचवड\", \"परिसरातील\", \"तरुणांना\", \"एकत्र\", \"करून\", \"निसर्ग\", \"संवर्धनाची\", \"कार्ये\", \"विविध\", \"उपक्रमांद्वारे\", \"करत\", \"आहे\", \".\"],\n              [\"कल्याण\", \"-\", \"डोंबिवली\", \"मध्ये\", \"माथाडी\", \"कामगारही\", \"उद्या\", \"बंदमध्ये\", \"सहभागी\", \"होणार\", \"असून\", \"एक\", \"भव्य\", \"मोर्चाही\", \"काढण्यात\", \"येणार\", \"आहे\", \".\"],\n              [\"दोन्ही\", \"ट्रकचा\", \"अपघात\", \"एवढा\", \"भीषण\", \"होता\", \"की\", \"ट्रकच्या\", \"समोरील\", \"बाजूचा\", \"चक्काचुर\", \"होऊन\", \"त्यामध्ये\", \"ट्रकचालक\", \"अडकले\", \"होते\", \".\"],\n              [\"सिबिल\", \"या\", \"संस्थेने\", \"प्रसिद्ध\", \"केलेल्या\", \"माहितीनुसार\", \",\", \"2018\", \"मध्ये\", \"जाणूनबुजून\", \"कर्ज\", \"बुडविणाऱ्या\", \"मंडळींच्या\", \"संख्येत\", \"मोठ्या\", \"प्रमाणावर\", \"वाढ\", \"झाली\", \"आहे\", \".\"],\n              [\"श्रीराम\", \"लागू\", \"-\", \"मराठी\", \"रंगभूमीचे\", \"अनभिषिक्त\", \"सम्राट\", \",\", \"चतुरस्र\", \"अभिनेते\", \",\", \"परखड\", \".\", \".\", \".\", \"30\", \"डिसेंबरला\", \"होणाऱ्या\", \"शपथविधी\", \"सोहळ्याची\", \"विधानभवनात\", \"जोरदार\", \"तयारी\", \"सुरू\"],\n              [\"कडाप्पे\", \"अंगावर\", \"पडून\", \"दोन\", \"कामगारांचा\", \"जागीच\", \"मृत्यू\", \",\", \"चार\", \"जण\", \"गंभीर\"],\n              [\"मात्र\", \"सत्ताधा-यांनी\", \"या\", \"विरोधाला\", \"न\", \"जुमानता\", \"हा\", \"प्रस्ताव\", \"मंजूर\", \"केला\", \".\"],\n              [\"रजनीकांत\", \"यांच्यावर\", \"प्रेम\", \"करणाऱ्या\", \"त्याच्या\", \"चाहत्यांची\", \"संख्या\", \"खूप\", \"मोठी\", \"आहे\", \".\"],\n              [\"एक\", \"हजार\", \"आणि\", \"एक\", \"आहेत\", \"नवीन\", \"वर्ष\", \"परिस्थिती\", \"प्रौढांसाठी\", \"घरी\", \"सर्जनशीलता\", \"आणि\", \"कल्पकता\", \"प्रदर्शित\", \",\", \"आपण\", \"अविस्मरणीय\", \"काहीतरी\", \"अनुकूल\", \"कंपनी\", \"सामान्य\", \"साधना\", \"चालू\", \"शकत\", \"नाही\", \",\", \"म्हणून\", \".\"],\n              [\"नेपाळच्या\", \"सौनाली\", \"जवळील\", \"पडासरी\", \"भागातील\", \"एका\", \"कारखान्यात\", \"झालेल्या\", \"प्रचंड\", \"स्फोटांच्या\", \"झळा\", \"भारतीय\", \"सीमेपर्यंतही\", \"पोहोचल्या\", \"असून\", \"या\", \"भागात\", \"सीमा\", \"सुरक्षा\", \"दलाची\", \"गस्त\", \"वाढविली\", \"गेली\", \"आहे\", \".\"],\n              [\"तसेच\", \"शाळांना\", \"लागलेल्या\", \"सुट्टय़ा\", \",\", \"लग्न\", \"सराई\", \",\", \"जत्रा\", \",\", \"यात्रा\", \".\", \".\", \".\", \"तीन\", \"ग्रामपंचायतीसाठी\", \"आज\", \"मतदान\"],\n              [\"जग\", \"आपण\", \"सुमारे\", \"गोल\", \"फिरणे\", \"नाही\", \",\", \"तो\", \"एक\", \"तारीख\", \"लक्षात\", \"विशेषतः\", \"महत्वाचे\", \"आहे\", \".\"],\n              [\"देशाचे\", \"विभाजन\", \"करणाऱ्यांनाच\", \"नव्हे\", \"तर\", \"अशा\", \"लोकांशी\", \"हातमिळवणी\", \"करणाऱ्यांनाही\", \"देशाच्या\", \"सत्ताकारणात\", \"यापुढे\", \"थारा\", \"मिळू\", \"नये\", \".\"],\n              [\"माजी\", \"जागतिक\", \"उपविजेता\", \"मुंबईचा\", \"दुसरा\", \"मानांकित\", \"मोहम्मद\", \"गुफरानने\", \"मुबंइ\", \"उपनगरच्या\", \"इश्तियाक\", \"अन्सारीचे\", \"25\", \"-\", \"6\", \",\", \"25\", \"-\", \"8\", \"असे\", \"आव्हान\", \"परतवून\", \"लावले\", \".\"],\n              [\"तथापि\", \",\", \"एक\", \"दीर्घ\", \"परंपरा\", \"आणि\", \"काम\", \"नीतिविषयक\", \"लोभ\", \"उद्ध्वस्त\", \"होते\", \".\"],\n              [\"सरकारने\", \"या\", \"अर्थसंकल्पीय\", \"अधिवेशनात\", \"शेतकऱ्यांना\", \"संपूर्ण\", \"कर्जमाफीची\", \"घोषणा\", \"करावी\", \",\", \"या\", \"मागणीसाठी\", \"विरोधक\", \"आज\", \"आक्रमक\", \"झाले\", \".\"],\n              [\"काँग्रेसचे\", \"विरोधक\", \"हे\", \"काँग्रेस\", \"मुक्त\", \"भारताची\", \"भाषा\", \"करतात\", \"तर\", \"संघाचे\", \"विरोधक\", \"संघ\", \"मुक्त\", \"भारताची\", \".\"]],\n\n   'ner_tags':[[\"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], \n             [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\"], #4\n             [\"O\", \"B-PER\", \"I-PER\", \"B-PER\", \"I-PER\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"B-PER\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #6\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"B-ORG\", \"I-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #9\n             [\"B-LOC\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #10\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #11\n             [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #12\n             [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #13\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #14\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"B-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #17\n             [\"B-LOC\", \"B-LOC\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"O\"], #19\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #22\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #24\n             [\"B-ORG\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"O\"]]\n    }","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:27:42.965997Z","iopub.execute_input":"2024-03-13T13:27:42.966348Z","iopub.status.idle":"2024-03-13T13:27:43.013204Z","shell.execute_reply.started":"2024-03-13T13:27:42.966320Z","shell.execute_reply":"2024-03-13T13:27:43.012116Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#NER_labels = ['B-ORG', 'B-MISC', 'I-ORG', 'I-LOC', 'B-LOC', 'I-MISC', 'I-PER', 'O', 'B-PER']\nNER_labels = ['B-ORG', 'I-ORG', 'I-LOC', 'B-LOC', 'I-PER', 'O', 'B-PER']\nlabel = ClassLabel(names=NER_labels,num_classes=7)\nsequence_feature = Sequence(feature=label, length=-1)\nprint(sequence_feature)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:27:52.355343Z","iopub.execute_input":"2024-03-13T13:27:52.356352Z","iopub.status.idle":"2024-03-13T13:27:52.364913Z","shell.execute_reply.started":"2024-03-13T13:27:52.356313Z","shell.execute_reply":"2024-03-13T13:27:52.363493Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Sequence(feature=ClassLabel(num_classes=7, names=['B-ORG', 'I-ORG', 'I-LOC', 'B-LOC', 'I-PER', 'O', 'B-PER'], id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"features = Features({\"tokens\": Sequence(feature=Value(dtype='string', id=None)), \"ner_tags\": sequence_feature})\ndataset = Dataset.from_dict(Q1_data, features=features)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:27:54.965333Z","iopub.execute_input":"2024-03-13T13:27:54.966175Z","iopub.status.idle":"2024-03-13T13:27:54.981997Z","shell.execute_reply.started":"2024-03-13T13:27:54.966138Z","shell.execute_reply":"2024-03-13T13:27:54.980888Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"Q1_dataset = dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on Q1 dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:27:59.291688Z","iopub.execute_input":"2024-03-13T13:27:59.292099Z","iopub.status.idle":"2024-03-13T13:27:59.714336Z","shell.execute_reply.started":"2024-03-13T13:27:59.292064Z","shell.execute_reply":"2024-03-13T13:27:59.713021Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5528ae8b3ab34d2d813de24cdb76e0ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d90c357250444c8cbe3ed161cf7b0fcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f833998d9b8d4ee2a994755e6b41ff17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9e4f9fbc1d94684abf7ab1ef93c70be"}},"metadata":{}}]},{"cell_type":"code","source":"a = trainer.predict(Q1_dataset)\na.metrics","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:28:02.901233Z","iopub.execute_input":"2024-03-13T13:28:02.901694Z","iopub.status.idle":"2024-03-13T13:28:03.500078Z","shell.execute_reply.started":"2024-03-13T13:28:02.901653Z","shell.execute_reply":"2024-03-13T13:28:03.498816Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.15\t\t0.01\t0.01\nB-ORG\t0.00\t\t0.00\t0.00\nB-PER\t0.00\t\t0.00\t0.00\nI-LOC\t0.00\t\t0.00\t0.00\nI-ORG\t0.00\t\t0.00\t0.00\nI-PER\t0.00\t\t0.00\t0.00\nO\t0.03\t\t0.74\t0.05\nmacro f1:\t0.00\t\t0.00\t0.03\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 5.947450637817383,\n 'test_LOC': {'precision': 0.14705882352941177,\n  'recall': 0.006165228113440197,\n  'f1': 0.011834319526627219,\n  'number': 811},\n 'test_ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 49},\n 'test_PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 16},\n 'test_overall_precision': 0.08928571428571429,\n 'test_overall_recall': 0.005707762557077625,\n 'test_overall_f1': 0.01072961373390558,\n 'test_overall_accuracy': 0.029473684210526315,\n 'test_runtime': 0.5809,\n 'test_samples_per_second': 43.039,\n 'test_steps_per_second': 3.443}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}