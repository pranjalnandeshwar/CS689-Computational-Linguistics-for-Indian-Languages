{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7810462,"sourceType":"datasetVersion","datasetId":4574587}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Marathi Dataset\n\nThe dataset for marathi language is directly fetched from the website.","metadata":{}},{"cell_type":"code","source":"!pip install datasets\nfrom datasets import ClassLabel, load_dataset, load_metric\n\nmr_dataset = load_dataset('ai4bharat/naamapadam','mr')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:24:41.063727Z","iopub.execute_input":"2024-03-12T19:24:41.064000Z","iopub.status.idle":"2024-03-12T19:26:11.601750Z","shell.execute_reply.started":"2024-03-12T19:24:41.063976Z","shell.execute_reply":"2024-03-12T19:26:11.600695Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d01ecb11e5d43688c11037f5d4738eb"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset naamapadam_pr/mr to /root/.cache/huggingface/datasets/ai4bharat___naamapadam_pr/mr/1.0.0/99b5ec77eabfaa3fbff510d8cf70d7c34519486cb7dbee99ede19474ddff9b20...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/25.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96836584d0c44f883d423c6448b1464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset naamapadam_pr downloaded and prepared to /root/.cache/huggingface/datasets/ai4bharat___naamapadam_pr/mr/1.0.0/99b5ec77eabfaa3fbff510d8cf70d7c34519486cb7dbee99ede19474ddff9b20. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d447fdd0c64476949d4d466ee893ac"}},"metadata":{}}]},{"cell_type":"code","source":"features = mr_dataset[\"train\"].features\nlabel_list = features['ner_tags'].feature.names\nlabel_to_id = {label_list[i]: features['ner_tags'].feature.str2int( label_list[i] ) for i in range(len(label_list))}\nprint(label_to_id)\nnum_labels = len(label_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:26:11.605679Z","iopub.execute_input":"2024-03-12T19:26:11.606405Z","iopub.status.idle":"2024-03-12T19:26:11.615134Z","shell.execute_reply.started":"2024-03-12T19:26:11.606366Z","shell.execute_reply":"2024-03-12T19:26:11.613519Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# FineTuning IndicNER for NER classification\n\nFirstly, we need to tokenize the training and validation dataset and then map the words to their NER labels.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification\nimport numpy as np\n\nconfig = AutoConfig.from_pretrained('ai4bharat/IndicNER', num_labels=num_labels, finetuning_task='ner')\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicNER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"ai4bharat/IndicNER\",num_labels=num_labels )","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:26:11.616622Z","iopub.execute_input":"2024-03-12T19:26:11.616970Z","iopub.status.idle":"2024-03-12T19:26:41.135949Z","shell.execute_reply.started":"2024-03-12T19:26:11.616940Z","shell.execute_reply":"2024-03-12T19:26:41.135139Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-03-12 19:26:19.861344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-12 19:26:19.861459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-12 19:26:19.994964: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291a11fb34f34042a2f6c37ebb29c1ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ace8c2a21c4405cac67326e93f80971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6575605540b4ab8b039c96ae98caae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad1a6c9e96cd42529efcb2c969568655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e232b25abc4721840719dbc1fc7b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/667M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b53c2a205294d2d8aa6cfab860c8c61"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:26:41.138283Z","iopub.execute_input":"2024-03-12T19:26:41.138857Z","iopub.status.idle":"2024-03-12T19:26:55.328470Z","shell.execute_reply.started":"2024-03-12T19:26:41.138821Z","shell.execute_reply":"2024-03-12T19:26:55.327498Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nCollecting accelerate\n  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.27.2\n    Uninstalling accelerate-0.27.2:\n      Successfully uninstalled accelerate-0.27.2\nSuccessfully installed accelerate-0.28.0\n","output_type":"stream"}]},{"cell_type":"code","source":"model=model.to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:26:55.329722Z","iopub.execute_input":"2024-03-12T19:26:55.330007Z","iopub.status.idle":"2024-03-12T19:26:55.712101Z","shell.execute_reply.started":"2024-03-12T19:26:55.329972Z","shell.execute_reply":"2024-03-12T19:26:55.711125Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Tokenize all texts and align the labels with them.\npadding = \"max_length\"\ndef tokenize_and_align_labels(examples): \n    tokenized_inputs = tokenizer(\n        examples['tokens'],\n        padding=padding,\n        truncation=True,\n        max_length=512,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    labels = []\n    for i, label in enumerate(examples['ner_tags']):       \n        word_ids = tokenized_inputs.word_ids(batch_index=i) \n\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:       \n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:26:55.713238Z","iopub.execute_input":"2024-03-12T19:26:55.713517Z","iopub.status.idle":"2024-03-12T19:26:55.721891Z","shell.execute_reply.started":"2024-03-12T19:26:55.713494Z","shell.execute_reply":"2024-03-12T19:26:55.720832Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing train_dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = mr_dataset[\"train\"].select(range(20000))   #sampling\n\ntrain_dataset = train_dataset.map(            \n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on train dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:26:55.723383Z","iopub.execute_input":"2024-03-12T19:26:55.723617Z","iopub.status.idle":"2024-03-12T19:27:01.631679Z","shell.execute_reply.started":"2024-03-12T19:26:55.723597Z","shell.execute_reply":"2024-03-12T19:27:01.630699Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #0:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d83f95c7643a4d0f993a8e4c91b21197"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #1:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2c8d0ddb04c48c89b5b5265cffb2f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #3:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1452447989df44aa9277fcf1b6287895"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #2:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60dd4f8693be4c538e20b99da1af99a3"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing validation_dataset\n\n\n","metadata":{}},{"cell_type":"code","source":"eval_dataset = mr_dataset[\"validation\"]        #no-sampling\neval_dataset = eval_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on Validation dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:01.633121Z","iopub.execute_input":"2024-03-12T19:27:01.633414Z","iopub.status.idle":"2024-03-12T19:27:02.641747Z","shell.execute_reply.started":"2024-03-12T19:27:01.633388Z","shell.execute_reply":"2024-03-12T19:27:02.640793Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de8cf24b78e74325b65cbbdfe03fc36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc2afcd1748c4d6dab7276897d6f85c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627dd70506324d5ab9e4099325c457d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bc1d6a1871140cab80bdfafce526cf6"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:02.643093Z","iopub.execute_input":"2024-03-12T19:27:02.643361Z","iopub.status.idle":"2024-03-12T19:27:02.647812Z","shell.execute_reply.started":"2024-03-12T19:27:02.643329Z","shell.execute_reply":"2024-03-12T19:27:02.647007Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Designing a compute metrics\n\nWe require compute metrics to evaluate the model. I have retrieved precision, recall, f1-score and accuracy here. Using these values, I will calculate the macro-f1 score.","metadata":{}},{"cell_type":"code","source":"!pip install seqeval\n\ndef all_classes(true_tags, pred_tags):\n    classes = set(tag for row in true_tags + pred_tags for tag in row)\n\n    results = []\n    true_positives, false_positives, false_negatives = 0, 0, 0\n    for class_label in sorted(classes):\n        true_indices = [(i, j) for i, row in enumerate(true_tags) for j, tag in enumerate(row) if tag == class_label]\n        pred_indices = [(i, j) for i, row in enumerate(pred_tags) for j, tag in enumerate(row) if tag == class_label]\n        \n        true_positive = len(set(true_indices) & set(pred_indices))\n        false_positive = len(set(pred_indices) - set(true_indices))\n        false_negative = len(set(true_indices) - set(pred_indices))\n\n        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n        results.append((class_label, precision, recall, f1_score))\n\n        true_positives += true_positive\n        false_positives += false_positive\n        false_negatives += false_negative\n\n    macro_f1_score = 2 * (true_positives) / (2 * true_positives + false_positives + false_negatives) if (2 * true_positives + false_positives + false_negatives) > 0 else 0\n    results.append(('macro f1:', 0, 0, macro_f1_score))\n\n    return results\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:02.651324Z","iopub.execute_input":"2024-03-12T19:27:02.651603Z","iopub.status.idle":"2024-03-12T19:27:18.134318Z","shell.execute_reply.started":"2024-03-12T19:27:02.651580Z","shell.execute_reply":"2024-03-12T19:27:18.133220Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=7267398d720461c551d5c413915b845cb70befd915309c9191e32a0d50832e90\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Metrics function\n#!pip install seqeval\nmetric = load_metric(\"seqeval\")\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    r1 = all_classes(true_labels, true_predictions)\n    print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n    for class_label, precision, recall, f1_score in r1:\n        print(f\"{class_label}\\t{precision:.2f}\\t\\t{recall:.2f}\\t{f1_score:.2f}\")\n    \n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:18.135951Z","iopub.execute_input":"2024-03-12T19:27:18.136348Z","iopub.status.idle":"2024-03-12T19:27:19.473188Z","shell.execute_reply.started":"2024-03-12T19:27:18.136313Z","shell.execute_reply":"2024-03-12T19:27:19.472320Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3e6e22e21d441385c8d6daf3eefcbe"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Finetuning the hyper-parameters\n\nFor fine-tuning a pre-trained model, we need to tune the hyper-parameters which are mentioned in the TrainingArguments. If a particular hyper parameter is not mentioned then it will take the default value.","metadata":{}},{"cell_type":"code","source":"args=TrainingArguments(\n    output_dir='Trained Models/IndicNER/Fine_tuned_IndicNER',\n    num_train_epochs=3,\n    logging_steps=500, #500\n    learning_rate=7e-5,\n    save_steps=1000, #1000\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:19.474454Z","iopub.execute_input":"2024-03-12T19:27:19.474731Z","iopub.status.idle":"2024-03-12T19:27:19.486907Z","shell.execute_reply.started":"2024-03-12T19:27:19.474708Z","shell.execute_reply":"2024-03-12T19:27:19.486047Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training the model\n\nAs training was taking more time and due to limited resources, I sampled off the training dataset. Also, I discussed this issue with my colleagues. They were getting different EFT (Expected Finish Time).","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,  #.select(range(20000)),   #sampling\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    args=args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:19.487925Z","iopub.execute_input":"2024-03-12T19:27:19.488199Z","iopub.status.idle":"2024-03-12T19:27:20.418300Z","shell.execute_reply.started":"2024-03-12T19:27:19.488176Z","shell.execute_reply":"2024-03-12T19:27:20.417404Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#GPU 100,000 -> 12h , 10k -> 3h, entire dataset -> 61h, 25% -> 15h\n#GPU Kaggle p100: 100,000 -> 7h,\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T19:27:20.419536Z","iopub.execute_input":"2024-03-12T19:27:20.419828Z","iopub.status.idle":"2024-03-12T20:23:01.285869Z","shell.execute_reply.started":"2024-03-12T19:27:20.419802Z","shell.execute_reply":"2024-03-12T20:23:01.285004Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240312_192728-wex4m8up</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/self-team-it/huggingface/runs/wex4m8up' target=\"_blank\">jumping-dream-14</a></strong> to <a href='https://wandb.ai/self-team-it/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/self-team-it/huggingface' target=\"_blank\">https://wandb.ai/self-team-it/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/self-team-it/huggingface/runs/wex4m8up' target=\"_blank\">https://wandb.ai/self-team-it/huggingface/runs/wex4m8up</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3750/3750 54:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.404000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.267700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.230200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.184500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.190500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.121900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.118700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3750, training_loss=0.21019397430419923, metrics={'train_runtime': 3340.4443, 'train_samples_per_second': 17.962, 'train_steps_per_second': 1.123, 'total_flos': 1.567851411456e+16, 'train_loss': 0.21019397430419923, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(train_dataset)\n#takes time","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:23:01.287513Z","iopub.execute_input":"2024-03-12T20:23:01.287905Z","iopub.status.idle":"2024-03-12T20:29:58.340396Z","shell.execute_reply.started":"2024-03-12T20:23:01.287865Z","shell.execute_reply":"2024-03-12T20:29:58.338615Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.93\t\t0.96\t0.94\nB-ORG\t0.94\t\t0.92\t0.93\nB-PER\t0.96\t\t0.96\t0.96\nI-LOC\t0.88\t\t0.84\t0.86\nI-ORG\t0.92\t\t0.94\t0.93\nI-PER\t0.94\t\t0.96\t0.95\nO\t0.99\t\t0.99\t0.99\nmacro f1:\t0.00\t\t0.00\t0.97\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.07519374042749405,\n 'eval_precision': 0.9235974700467938,\n 'eval_recall': 0.934593250148297,\n 'eval_macro-f1': 0.929062826521006,\n 'eval_accuracy': 0.9745609859966432,\n 'eval_runtime': 417.0117,\n 'eval_samples_per_second': 47.96,\n 'eval_steps_per_second': 2.998,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"code","source":"#GPU\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:29:58.341822Z","iopub.execute_input":"2024-03-12T20:29:58.343600Z","iopub.status.idle":"2024-03-12T20:30:46.324052Z","shell.execute_reply.started":"2024-03-12T20:29:58.343571Z","shell.execute_reply":"2024-03-12T20:30:46.323045Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.81\t\t0.86\t0.84\nB-ORG\t0.76\t\t0.76\t0.76\nB-PER\t0.86\t\t0.88\t0.87\nI-LOC\t0.69\t\t0.65\t0.67\nI-ORG\t0.71\t\t0.71\t0.71\nI-PER\t0.88\t\t0.88\t0.88\nO\t0.96\t\t0.95\t0.96\nmacro f1:\t0.00\t\t0.00\t0.92\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.31304654479026794,\n 'eval_precision': 0.7948739355485056,\n 'eval_recall': 0.8211297973264338,\n 'eval_macro-f1': 0.8077885716709794,\n 'eval_accuracy': 0.9185480702390667,\n 'eval_runtime': 47.9606,\n 'eval_samples_per_second': 47.956,\n 'eval_steps_per_second': 3.002,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"Fine_tuned_IndicNER\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:30:46.325725Z","iopub.execute_input":"2024-03-12T20:30:46.326524Z","iopub.status.idle":"2024-03-12T20:30:48.142485Z","shell.execute_reply.started":"2024-03-12T20:30:46.326491Z","shell.execute_reply":"2024-03-12T20:30:48.141367Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Test Phase","metadata":{}},{"cell_type":"code","source":"#TEST\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/Fine_tuned_IndicNER\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/Fine_tuned_IndicNER\")\n\ntest_dataset = mr_dataset[\"test\"]\ntest_dataset = test_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on test dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:49.942824Z","iopub.execute_input":"2024-03-12T20:36:49.943125Z","iopub.status.idle":"2024-03-12T20:36:51.106901Z","shell.execute_reply.started":"2024-03-12T20:36:49.943074Z","shell.execute_reply":"2024-03-12T20:36:51.105747Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/Fine_tuned_IndicNER and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65be014094fd4616bf6a60f8ba23c5d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b8a7170f0e4cfbb2cbbfec4f1eb5b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30b99e13fa74b529dc91559ac8e2cd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fbb57d4c72c4f359ec25129b4dee54f"}},"metadata":{}}]},{"cell_type":"code","source":"x = trainer.predict(test_dataset)\nx.metrics","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:30:49.287337Z","iopub.execute_input":"2024-03-12T20:30:49.287761Z","iopub.status.idle":"2024-03-12T20:31:11.807635Z","shell.execute_reply.started":"2024-03-12T20:30:49.287716Z","shell.execute_reply":"2024-03-12T20:31:11.806658Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.88\t\t0.81\t0.85\nB-ORG\t0.75\t\t0.73\t0.74\nB-PER\t0.90\t\t0.88\t0.89\nI-LOC\t0.67\t\t0.68\t0.68\nI-ORG\t0.65\t\t0.60\t0.63\nI-PER\t0.94\t\t0.93\t0.93\nO\t0.95\t\t0.96\t0.96\nmacro f1:\t0.00\t\t0.00\t0.92\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3161526620388031,\n 'test_precision': 0.8255954784012919,\n 'test_recall': 0.7971155720132528,\n 'test_macro-f1': 0.8111056023797718,\n 'test_accuracy': 0.92080751104019,\n 'test_runtime': 22.5027,\n 'test_samples_per_second': 47.994,\n 'test_steps_per_second': 3.022}"},"metadata":{}}]},{"cell_type":"code","source":"macro_f1_test = x.metrics['test_macro-f1']\nprint(\"Macro f1 score on test dataset: \",macro_f1_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:31:11.808938Z","iopub.execute_input":"2024-03-12T20:31:11.809311Z","iopub.status.idle":"2024-03-12T20:31:11.816225Z","shell.execute_reply.started":"2024-03-12T20:31:11.809286Z","shell.execute_reply":"2024-03-12T20:31:11.815139Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Macro f1 score on test dataset:  0.8111056023797718\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:31:11.817932Z","iopub.execute_input":"2024-03-12T20:31:11.818265Z","iopub.status.idle":"2024-03-12T20:36:47.815324Z","shell.execute_reply.started":"2024-03-12T20:31:11.818235Z","shell.execute_reply":"2024-03-12T20:36:47.814042Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"updating: kaggle/working/ (stored 0%)\nupdating: kaggle/working/Trained Models/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/Mar12_16-46-10_83339e7580ca/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/Mar12_16-46-10_83339e7580ca/events.out.tfevents.1710265729.83339e7580ca.34.1 (deflated 49%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/Mar12_16-46-10_83339e7580ca/events.out.tfevents.1710261971.83339e7580ca.34.0 (deflated 60%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/tokenizer.json (deflated 69%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/training_args.bin (deflated 51%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/rng_state.pth (deflated 25%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/model.safetensors (deflated 7%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/trainer_state.json (deflated 62%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/tokenizer_config.json (deflated 75%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/optimizer.pt (deflated 52%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/vocab.txt (deflated 48%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/scheduler.pt (deflated 56%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/special_tokens_map.json (deflated 80%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-2000/config.json (deflated 54%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/tokenizer.json (deflated 69%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/training_args.bin (deflated 51%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/rng_state.pth (deflated 25%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/model.safetensors (deflated 7%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/trainer_state.json (deflated 66%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/tokenizer_config.json (deflated 75%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/optimizer.pt (deflated 52%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/vocab.txt (deflated 48%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/scheduler.pt (deflated 55%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/special_tokens_map.json (deflated 80%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-3000/config.json (deflated 54%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/ (stored 0%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/tokenizer.json (deflated 69%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/training_args.bin (deflated 51%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/rng_state.pth (deflated 25%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/model.safetensors (deflated 7%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/trainer_state.json (deflated 55%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/tokenizer_config.json (deflated 75%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/optimizer.pt (deflated 52%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/vocab.txt (deflated 48%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/scheduler.pt (deflated 55%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/special_tokens_map.json (deflated 80%)\nupdating: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/checkpoint-1000/config.json (deflated 54%)\nupdating: kaggle/working/.virtual_documents/ (stored 0%)\nupdating: kaggle/working/wandb/ (stored 0%)\nupdating: kaggle/working/wandb/debug-internal.log (deflated 94%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/run-5grqy37v.wandb (deflated 82%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/logs/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/logs/debug-internal.log (deflated 94%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/logs/debug.log (deflated 81%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/ (stored 0%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/output.log (deflated 92%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/config.yaml (deflated 77%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/requirements.txt (deflated 58%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/conda-environment.yaml (deflated 67%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/wandb-metadata.json (deflated 65%)\nupdating: kaggle/working/wandb/run-20240312_164632-5grqy37v/files/wandb-summary.json (deflated 52%)\nupdating: kaggle/working/wandb/latest-run/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/tmp/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/tmp/code/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/logs/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/logs/debug-internal.log (deflated 94%)\nupdating: kaggle/working/wandb/latest-run/logs/debug.log (deflated 71%)\nupdating: kaggle/working/wandb/latest-run/files/ (stored 0%)\nupdating: kaggle/working/wandb/latest-run/files/output.log (deflated 84%)\nupdating: kaggle/working/wandb/latest-run/files/config.yaml (deflated 77%)\nupdating: kaggle/working/wandb/latest-run/files/requirements.txt (deflated 58%)\nupdating: kaggle/working/wandb/latest-run/files/conda-environment.yaml (deflated 67%)\nupdating: kaggle/working/wandb/latest-run/files/wandb-metadata.json (deflated 65%)\nupdating: kaggle/working/wandb/latest-run/files/wandb-summary.json (deflated 52%)\nupdating: kaggle/working/wandb/debug.log (deflated 71%)\nupdating: kaggle/working/Fine_tuned_IndicNER/ (stored 0%)\nupdating: kaggle/working/Fine_tuned_IndicNER/tokenizer.json (deflated 69%)\nupdating: kaggle/working/Fine_tuned_IndicNER/training_args.bin (deflated 51%)\nupdating: kaggle/working/Fine_tuned_IndicNER/model.safetensors (deflated 7%)\nupdating: kaggle/working/Fine_tuned_IndicNER/tokenizer_config.json (deflated 75%)\nupdating: kaggle/working/Fine_tuned_IndicNER/vocab.txt (deflated 48%)\nupdating: kaggle/working/Fine_tuned_IndicNER/special_tokens_map.json (deflated 80%)\nupdating: kaggle/working/Fine_tuned_IndicNER/config.json (deflated 54%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/tmp/code/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/requirements.txt (deflated 58%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/wandb-metadata.json (deflated 65%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/config.yaml (deflated 77%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/output.log (deflated 87%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/conda-environment.yaml (deflated 67%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/files/wandb-summary.json (deflated 52%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/logs/ (stored 0%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/logs/debug-internal.log (deflated 94%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/logs/debug.log (deflated 71%)\n  adding: kaggle/working/wandb/run-20240312_192728-wex4m8up/run-wex4m8up.wandb (deflated 82%)\n  adding: kaggle/working/wandb/latest-run/run-wex4m8up.wandb (deflated 82%)\n  adding: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/Mar12_19-27-19_98f531e30490/ (stored 0%)\n  adding: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/Mar12_19-27-19_98f531e30490/events.out.tfevents.1710275398.98f531e30490.34.1 (deflated 48%)\n  adding: kaggle/working/Trained Models/IndicNER/Fine_tuned_IndicNER/runs/Mar12_19-27-19_98f531e30490/events.out.tfevents.1710271640.98f531e30490.34.0 (deflated 60%)\n  adding: kaggle/working/state.db (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:47.817455Z","iopub.execute_input":"2024-03-12T20:36:47.817915Z","iopub.status.idle":"2024-03-12T20:36:48.849875Z","shell.execute_reply.started":"2024-03-12T20:36:47.817872Z","shell.execute_reply":"2024-03-12T20:36:48.848673Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":" Fine_tuned_IndicNER  'Trained Models'\t file.zip   state.db   wandb\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:48.851656Z","iopub.execute_input":"2024-03-12T20:36:48.852069Z","iopub.status.idle":"2024-03-12T20:36:48.861005Z","shell.execute_reply.started":"2024-03-12T20:36:48.852030Z","shell.execute_reply":"2024-03-12T20:36:48.859823Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Q4 Appended.\n\nThe part of Question 4 wherein we have to compute precision recall f1-score on the fine-tuned IndicNER has been appended here.","metadata":{}},{"cell_type":"code","source":"from datasets import ClassLabel, Sequence, Dataset, Value, Features, load_dataset, load_metric\nQ1_data = {\n    'tokens':[[\"कार्यकर्त्यांमधील\", \"वाद\", \"बघून\", \"संतापलेल्या\", \"सुप्रिया\", \"सुळे\", \"यांनी\", \"कार्यक्रमामध्येच\", \"राडेबाज\", \"कार्यकर्त्यांना\",\"दम\", \"दिला\", \".\"],\n              [\"अर्थात\", \"यात\", \"काही\", \"चांगल्या\", \"आणि\", \"व्हिडिओला\", \"पाठिंबा\", \"देणाऱ्या\", \"कमेंटही\", \"होत्या\", \".\"],\n              [\"भारतीय\", \"वायुसेनेनं\", \"केलेल्या\", \"कारवाईचं\", \"संपुर्ण\", \"देशभरात\", \"कौतूक\", \"केलं\", \"जात\", \"आहे\", \".\"],\n              [\"24)\", \"बैठक\", \"बोलावली\", \"असल्याची\", \"माहिती\", \"आमदार\", \"अनिल\", \"बाबर\", \"यांनी\",\".\", \".\", \".\", \"ढवळीत\",\"पुरग्रस्तांच्या\",\"मदत\", \"वाटपाची\", \"चौकशी\", \"सुरु\"],\n              [\"यावेळी\", \"सुरेश\", \"प्रभुनी\", \"सुधीर\", \"सावंत\", \"व\", \"मधु\", \"दंडवते\", \"या\", \"दोघांचाही\", \"पराभव\", \"केला\", \".\", \"त्यावेळी\", \"निकालादिवशी\", \"सुरेश\", \"प्रभुंना\", \"विजयी\", \"घोषित\" \"केल्यानंतर\", \"त्यांनी\", \"लगेचच\", \"शेजारी\", \"उभे\", \"असलेल्या\", \"मधु\", \"दंडवतेंची\", \"भेट\", \"घेत\", \"वाकुन\", \"त्यांच्या\", \"चरणांना\", \"स्पर्श\", \"केला\", \".\", \"मतांच्या\", \"गोळाबेरजेत\", \"आपण\", \"विजयी\", \"झालेलो\", \"असलो\", \"तरी\", \"मधु\", \"दंडवतें\", \"या\", \"व्यक्तीचे\", \"कोकणप्रती\", \"असलेले\", \"योगदान\", \"प्रभुना\", \"चांगलेच\", \"ठाऊक\", \"होते\", \".\"],\n              [\"सव्वाशे\", \"वर्षे\", \"अखंड\", \"पाणीपुरवठा\", \":\", \"संस्थानकालीन\", \"योजना\", \":\", \"सुधारित\", \"45\", \"कोटींचा\", \"आराखडा\", \"लवकरच\", \"मंजूर\"],\n              [\"कार्यक्रम\", \"सर्वांना\", \"खुला\", \"असून\", \",\", \"रसिकांनी\", \"उपस्थित\", \"रहावे\", \"असे\", \"आवाहन\", \"लोकमान्य\", \"ग्रंथालयाने\", \"केले\", \"आहे\", \".\"],\n              [\"विमोचन\", \"आणि\", \"तो\", \"एक\", \"पादचारी\", \"दाबा\", \"आणि\", \"एक\", \"कार\", \"दाबा\", \"होते\", \"तेव्हा\", \",\", \"उदाहरणार्थ\", \",\", \"गुन्हा\", \"मागोवा\", \"लपविण्यासाठी\", \"मानले\", \"जाते\", \".\"],\n              [\"निसर्गराजा\", \"मित्र\", \"जीवांचे\", \"ही\", \"संस्था\", \"गेली\", \"दहा\", \"वर्षे\", \"पिंपरी\", \"-\", \"चिंचवड\", \"परिसरातील\", \"तरुणांना\", \"एकत्र\", \"करून\", \"निसर्ग\", \"संवर्धनाची\", \"कार्ये\", \"विविध\", \"उपक्रमांद्वारे\", \"करत\", \"आहे\", \".\"],\n              [\"कल्याण\", \"-\", \"डोंबिवली\", \"मध्ये\", \"माथाडी\", \"कामगारही\", \"उद्या\", \"बंदमध्ये\", \"सहभागी\", \"होणार\", \"असून\", \"एक\", \"भव्य\", \"मोर्चाही\", \"काढण्यात\", \"येणार\", \"आहे\", \".\"],\n              [\"दोन्ही\", \"ट्रकचा\", \"अपघात\", \"एवढा\", \"भीषण\", \"होता\", \"की\", \"ट्रकच्या\", \"समोरील\", \"बाजूचा\", \"चक्काचुर\", \"होऊन\", \"त्यामध्ये\", \"ट्रकचालक\", \"अडकले\", \"होते\", \".\"],\n              [\"सिबिल\", \"या\", \"संस्थेने\", \"प्रसिद्ध\", \"केलेल्या\", \"माहितीनुसार\", \",\", \"2018\", \"मध्ये\", \"जाणूनबुजून\", \"कर्ज\", \"बुडविणाऱ्या\", \"मंडळींच्या\", \"संख्येत\", \"मोठ्या\", \"प्रमाणावर\", \"वाढ\", \"झाली\", \"आहे\", \".\"],\n              [\"श्रीराम\", \"लागू\", \"-\", \"मराठी\", \"रंगभूमीचे\", \"अनभिषिक्त\", \"सम्राट\", \",\", \"चतुरस्र\", \"अभिनेते\", \",\", \"परखड\", \".\", \".\", \".\", \"30\", \"डिसेंबरला\", \"होणाऱ्या\", \"शपथविधी\", \"सोहळ्याची\", \"विधानभवनात\", \"जोरदार\", \"तयारी\", \"सुरू\"],\n              [\"कडाप्पे\", \"अंगावर\", \"पडून\", \"दोन\", \"कामगारांचा\", \"जागीच\", \"मृत्यू\", \",\", \"चार\", \"जण\", \"गंभीर\"],\n              [\"मात्र\", \"सत्ताधा-यांनी\", \"या\", \"विरोधाला\", \"न\", \"जुमानता\", \"हा\", \"प्रस्ताव\", \"मंजूर\", \"केला\", \".\"],\n              [\"रजनीकांत\", \"यांच्यावर\", \"प्रेम\", \"करणाऱ्या\", \"त्याच्या\", \"चाहत्यांची\", \"संख्या\", \"खूप\", \"मोठी\", \"आहे\", \".\"],\n              [\"एक\", \"हजार\", \"आणि\", \"एक\", \"आहेत\", \"नवीन\", \"वर्ष\", \"परिस्थिती\", \"प्रौढांसाठी\", \"घरी\", \"सर्जनशीलता\", \"आणि\", \"कल्पकता\", \"प्रदर्शित\", \",\", \"आपण\", \"अविस्मरणीय\", \"काहीतरी\", \"अनुकूल\", \"कंपनी\", \"सामान्य\", \"साधना\", \"चालू\", \"शकत\", \"नाही\", \",\", \"म्हणून\", \".\"],\n              [\"नेपाळच्या\", \"सौनाली\", \"जवळील\", \"पडासरी\", \"भागातील\", \"एका\", \"कारखान्यात\", \"झालेल्या\", \"प्रचंड\", \"स्फोटांच्या\", \"झळा\", \"भारतीय\", \"सीमेपर्यंतही\", \"पोहोचल्या\", \"असून\", \"या\", \"भागात\", \"सीमा\", \"सुरक्षा\", \"दलाची\", \"गस्त\", \"वाढविली\", \"गेली\", \"आहे\", \".\"],\n              [\"तसेच\", \"शाळांना\", \"लागलेल्या\", \"सुट्टय़ा\", \",\", \"लग्न\", \"सराई\", \",\", \"जत्रा\", \",\", \"यात्रा\", \".\", \".\", \".\", \"तीन\", \"ग्रामपंचायतीसाठी\", \"आज\", \"मतदान\"],\n              [\"जग\", \"आपण\", \"सुमारे\", \"गोल\", \"फिरणे\", \"नाही\", \",\", \"तो\", \"एक\", \"तारीख\", \"लक्षात\", \"विशेषतः\", \"महत्वाचे\", \"आहे\", \".\"],\n              [\"देशाचे\", \"विभाजन\", \"करणाऱ्यांनाच\", \"नव्हे\", \"तर\", \"अशा\", \"लोकांशी\", \"हातमिळवणी\", \"करणाऱ्यांनाही\", \"देशाच्या\", \"सत्ताकारणात\", \"यापुढे\", \"थारा\", \"मिळू\", \"नये\", \".\"],\n              [\"माजी\", \"जागतिक\", \"उपविजेता\", \"मुंबईचा\", \"दुसरा\", \"मानांकित\", \"मोहम्मद\", \"गुफरानने\", \"मुबंइ\", \"उपनगरच्या\", \"इश्तियाक\", \"अन्सारीचे\", \"25\", \"-\", \"6\", \",\", \"25\", \"-\", \"8\", \"असे\", \"आव्हान\", \"परतवून\", \"लावले\", \".\"],\n              [\"तथापि\", \",\", \"एक\", \"दीर्घ\", \"परंपरा\", \"आणि\", \"काम\", \"नीतिविषयक\", \"लोभ\", \"उद्ध्वस्त\", \"होते\", \".\"],\n              [\"सरकारने\", \"या\", \"अर्थसंकल्पीय\", \"अधिवेशनात\", \"शेतकऱ्यांना\", \"संपूर्ण\", \"कर्जमाफीची\", \"घोषणा\", \"करावी\", \",\", \"या\", \"मागणीसाठी\", \"विरोधक\", \"आज\", \"आक्रमक\", \"झाले\", \".\"],\n              [\"काँग्रेसचे\", \"विरोधक\", \"हे\", \"काँग्रेस\", \"मुक्त\", \"भारताची\", \"भाषा\", \"करतात\", \"तर\", \"संघाचे\", \"विरोधक\", \"संघ\", \"मुक्त\", \"भारताची\", \".\"]],\n\n   'ner_tags':[[\"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], \n             [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\"], #4\n             [\"O\", \"B-PER\", \"I-PER\", \"B-PER\", \"I-PER\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"B-PER\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #6\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"B-ORG\", \"I-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #9\n             [\"B-LOC\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #10\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #11\n             [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #12\n             [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #13\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #14\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"B-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #17\n             [\"B-LOC\", \"B-LOC\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"O\"], #19\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #22\n             [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n             [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], #24\n             [\"B-ORG\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"O\"]]\n    }","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:48.862697Z","iopub.execute_input":"2024-03-12T20:36:48.862998Z","iopub.status.idle":"2024-03-12T20:36:48.908485Z","shell.execute_reply.started":"2024-03-12T20:36:48.862964Z","shell.execute_reply":"2024-03-12T20:36:48.907576Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#NER_labels = ['B-ORG', 'B-MISC', 'I-ORG', 'I-LOC', 'B-LOC', 'I-MISC', 'I-PER', 'O', 'B-PER']\nNER_labels = ['B-ORG', 'I-ORG', 'I-LOC', 'B-LOC', 'I-PER', 'O', 'B-PER']\nlabel = ClassLabel(names=NER_labels,num_classes=7)\nsequence_feature = Sequence(feature=label, length=-1)\nprint(sequence_feature)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:48.909715Z","iopub.execute_input":"2024-03-12T20:36:48.910404Z","iopub.status.idle":"2024-03-12T20:36:48.923037Z","shell.execute_reply.started":"2024-03-12T20:36:48.910371Z","shell.execute_reply":"2024-03-12T20:36:48.922118Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Sequence(feature=ClassLabel(num_classes=7, names=['B-ORG', 'I-ORG', 'I-LOC', 'B-LOC', 'I-PER', 'O', 'B-PER'], id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"features = Features({\"tokens\": Sequence(feature=Value(dtype='string', id=None)), \"ner_tags\": sequence_feature})\ndataset = Dataset.from_dict(Q1_data, features=features)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:48.924231Z","iopub.execute_input":"2024-03-12T20:36:48.924490Z","iopub.status.idle":"2024-03-12T20:36:48.945591Z","shell.execute_reply.started":"2024-03-12T20:36:48.924467Z","shell.execute_reply":"2024-03-12T20:36:48.944497Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"Q1_dataset = dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on Q1 dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:36:48.946735Z","iopub.execute_input":"2024-03-12T20:36:48.947235Z","iopub.status.idle":"2024-03-12T20:36:49.342409Z","shell.execute_reply.started":"2024-03-12T20:36:48.947211Z","shell.execute_reply":"2024-03-12T20:36:49.341197Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b56f717254e4fe7a8e439d1b5ed5311"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5198765701643c29759ea027b38fc16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043e51196edc4bd2a13a0199f1b458dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Q1 dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4079c05449414bbb8639401e6d5c9980"}},"metadata":{}}]},{"cell_type":"code","source":"a = trainer.predict(Q1_dataset)\na.metrics","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:39:21.770535Z","iopub.execute_input":"2024-03-12T20:39:21.770846Z","iopub.status.idle":"2024-03-12T20:39:22.364297Z","shell.execute_reply.started":"2024-03-12T20:39:21.770819Z","shell.execute_reply":"2024-03-12T20:39:22.363162Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Class\tPrecision\tRecall\tF1-Score\nB-LOC\t0.18\t\t0.01\t0.01\nB-ORG\t0.00\t\t0.00\t0.00\nB-PER\t0.00\t\t0.00\t0.00\nI-LOC\t0.00\t\t0.00\t0.00\nI-ORG\t0.00\t\t0.00\t0.00\nI-PER\t0.00\t\t0.00\t0.00\nO\t0.01\t\t0.50\t0.03\nmacro f1:\t0.00\t\t0.00\t0.02\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 9.208431243896484,\n 'test_precision': 0.06593406593406594,\n 'test_recall': 0.006060606060606061,\n 'test_macro-f1': 0.011100832562442183,\n 'test_accuracy': 0.01858736059479554,\n 'test_runtime': 0.5754,\n 'test_samples_per_second': 43.452,\n 'test_steps_per_second': 3.476}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}